{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6b96ef-00f0-4cfc-a134-ba4908e4fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE: Clear cell output occasionally to optimize UI efficiency\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe93ebc-5e4b-4019-bcd5-7e28bba65a1a",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 1: Create catalog\n",
    "This script finds all .tt0.subim.fits files in VLASS epochs 2.1 and 2.2\n",
    "from https://archive-new.nrao.edu/vlass/se_continuum_imaging/ and records \n",
    "the target folder URL, epoch, tile name, and target name into a CSV file \n",
    "in the current working directory\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Base URL and epochs to scan\n",
    "top_url = \"https://archive-new.nrao.edu/vlass/se_continuum_imaging/\"\n",
    "epoch_dirs = [\"VLASS2.1/\", \"VLASS2.2/\"]\n",
    "\n",
    "# Patterns\n",
    "fits_pattern = re.compile(r\".*\\.tt0\\.subim\\.fits$\", re.IGNORECASE)\n",
    "target_pattern = re.compile(r\"[Jj]\\d{6}[+-]\\d{6}\")\n",
    "\n",
    "# Output file\n",
    "output_csv = \"epoch2_continuum_catalog.csv\"\n",
    "\n",
    "\n",
    "def load_processed(csv_file):\n",
    "    # Load processed data from existing CSV to skip them on resume\n",
    "    processed = set()\n",
    "    if os.path.exists(csv_file):\n",
    "        with open(csv_file, newline='') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                processed.add(row['Link'])\n",
    "    return processed\n",
    "\n",
    "\n",
    "def list_tiles(epoch_url):\n",
    "    # Return list of tile subdirectories under the given epoch\n",
    "    print(f\"[PARSE] Getting tiles in {epoch_url}\")\n",
    "    resp = requests.get(epoch_url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    return [a['href'] for a in soup.find_all('a', href=True)\n",
    "            if a['href'].endswith('/') and a['href'].startswith('T')]\n",
    "\n",
    "\n",
    "def list_targets(tile_url):\n",
    "    # Return list of target subdirectories under the given tile\n",
    "    print(f\"[PARSE] Getting targets in {tile_url}\")\n",
    "    resp = requests.get(tile_url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    return [a['href'] for a in soup.find_all('a', href=True)\n",
    "            if a['href'].endswith('/') and a['href'].lower().startswith('vlass2.') and '.se.' in a['href']]\n",
    "\n",
    "\n",
    "def process_target(epoch, tile, target_dir, tile_url, writer, processed):\n",
    "    # Process a target directory: if processed - skip; else - record its path data\n",
    "    target_url = urljoin(tile_url, target_dir)\n",
    "    if target_url in processed:\n",
    "        print(f\"[SKIP] Already processed {target_url}\")\n",
    "        return\n",
    "    print(f\"[PARSE] Opening {target_url}\")\n",
    "    resp = requests.get(target_url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        if fits_pattern.match(href):\n",
    "            # Write row for target's folder URL\n",
    "            folder_url = target_url\n",
    "            match = target_pattern.search(href)\n",
    "            target_name = match.group(0) if match else ''\n",
    "            writer.writerow([folder_url, epoch, tile, target_name])\n",
    "            print(f\"[FOUND] {folder_url} | Epoch {epoch} | Tile {tile} | Target {target_name}\")\n",
    "            processed.add(folder_url)\n",
    "            break\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Open CSV in append or write mode, iterate through epochs, tiles, targets\n",
    "    processed = load_processed(output_csv)\n",
    "    mode = 'a' if processed else 'w'\n",
    "\n",
    "    with open(output_csv, mode, newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        if mode == 'w':\n",
    "            writer.writerow(['Link', 'Epoch', 'Tile', 'Target'])\n",
    "\n",
    "        for epoch_dir in epoch_dirs:\n",
    "            epoch = epoch_dir.replace('VLASS', '').strip('/')\n",
    "            epoch_url = urljoin(top_url, epoch_dir)\n",
    "            for tile_dir in list_tiles(epoch_url):\n",
    "                tile = tile_dir.rstrip('/')\n",
    "                tile_url = urljoin(epoch_url, tile_dir)\n",
    "                for target_dir in list_targets(tile_url):\n",
    "                    process_target(epoch, tile, target_dir, tile_url, writer, processed)\n",
    "\n",
    "    print(f\"[COMPLETE] Created {output_csv}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb70db9f-823f-48f6-b811-89273b0e279e",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 2: Download .fits files\n",
    "Read the catalog CSV, open and scan each URL for .tt0.subim.fits files,\n",
    "and download them to a directory\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "# Output directory\n",
    "outdir = \"epoch2_continuum_fits\"\n",
    "\n",
    "def download_fits(csv_file=output_csv, outdir=outdir):\n",
    "    # Download all .tt0.subim.fits files listed in the catalog\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    with open(csv_file, newline='') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            folder_url = row['Link']\n",
    "            print(f\"[SCAN] {folder_url}\")\n",
    "            resp = requests.get(folder_url)\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "            # Find and download matching FITS files\n",
    "            for a in soup.find_all('a', href=True):\n",
    "                href = a['href']\n",
    "                if fits_pattern.match(href):\n",
    "                    file_url = urljoin(folder_url, href)\n",
    "                    filename = os.path.basename(href)\n",
    "                    dest = os.path.join(outdir, filename)\n",
    "\n",
    "                    if os.path.exists(dest):\n",
    "                        print(f\"[SKIP] {filename}\")\n",
    "                        continue\n",
    "\n",
    "                    print(f\"[GET] {file_url}\")\n",
    "                    file_resp = requests.get(file_url, stream=True)\n",
    "                    file_resp.raise_for_status()\n",
    "                    with open(dest, 'wb') as out_f:\n",
    "                        for chunk in file_resp.iter_content(chunk_size=8192):\n",
    "                            out_f.write(chunk)\n",
    "                    print(f\"[SAVE] {dest}\")\n",
    "\n",
    "def main_download():\n",
    "    # Download FITS using the catalog\n",
    "    download_fits()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main_download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2504929a-f8c3-4dcc-8bd4-24a1147b83f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 3: Get RA/Dec\n",
    "Iterate through each fits file, open the header with astropy to \n",
    "read data and parse RA/Dec in degrees and observation date/time \n",
    "data, and append to the catalog\n",
    "\"\"\"\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.wcs import WCS\n",
    "import astropy.units as u\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def update_catalog(csv_file=output_csv, fits_folder=outdir):\n",
    "    \"\"\"\n",
    "    Update the existing catalog and append columns for RA, Dec, Date Start, Date\n",
    "    End, Time Start, and Time End\n",
    "    \"\"\"\n",
    "    # Verify FITS directory exists\n",
    "    if not os.path.isdir(fits_folder):\n",
    "        print(f\"[ERROR] Not found: {fits_folder}\")\n",
    "        return\n",
    "\n",
    "    # Read existing CSV into memory\n",
    "    with open(csv_file, newline='') as f_in:\n",
    "        reader = csv.DictReader(f_in)\n",
    "        rows = list(reader)\n",
    "        # Determine which additional columns to add\n",
    "        new_cols = ['RA', 'Dec', 'Date Start', 'Date End', 'Time Start', 'Time End']\n",
    "        # Only append missing columns\n",
    "        fieldnames = reader.fieldnames + [col for col in new_cols if col not in reader.fieldnames]\n",
    "\n",
    "    # Overwrite original CSV with updated data\n",
    "    with open(csv_file, 'w', newline='') as f_out:\n",
    "        writer = csv.DictWriter(f_out, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in rows:\n",
    "            # Skip if already contains RA\n",
    "            if row.get('RA') not in (None, ''):\n",
    "                writer.writerow(row)\n",
    "                continue\n",
    "            target = row.get('Target', '')\n",
    "            # Find FITS\n",
    "            matches = [fn for fn in os.listdir(fits_folder)\n",
    "                       if fn.endswith('.tt0.subim.fits') and target in fn]\n",
    "\n",
    "            if matches:\n",
    "                filepath = os.path.join(fits_folder, matches[0])\n",
    "                try:\n",
    "                    # Read header for dates\n",
    "                    hdr = fits.getheader(filepath)\n",
    "                    date_obs = hdr.get('DATE-OBS', '')\n",
    "                    date_end = hdr.get('DATE-END', '')\n",
    "\n",
    "                    # Read full data array (auto‚Äêdecompresses if needed)\n",
    "                    data = fits.getdata(filepath, memmap=False)\n",
    "                    header2 = fits.getheader(filepath)\n",
    "                    data = np.squeeze(data)\n",
    "                    wcs_full = WCS(header2)\n",
    "                    try:\n",
    "                        wcs = wcs_full.celestial\n",
    "                    except Exception:\n",
    "                        wcs = wcs_full\n",
    "\n",
    "                    # Find brightest pixel and convert to sky coordinates\n",
    "                    linidx = np.nanargmax(data)\n",
    "                    y_pix, x_pix = np.unravel_index(linidx, data.shape)\n",
    "                    skycoord = wcs.pixel_to_world(x_pix, y_pix)\n",
    "                    ra = skycoord.ra.deg\n",
    "                    dec = skycoord.dec.deg\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] Cannot read {filepath}: {e}\")\n",
    "                    ra = dec = date_obs = date_end = ''\n",
    "                # Parse date and time separately\n",
    "                if 'T' in date_obs:\n",
    "                    obs_date, obs_time = date_obs.split('T', 1)\n",
    "                else:\n",
    "                    obs_date = obs_time = ''\n",
    "                if 'T' in date_end:\n",
    "                    end_date, end_time = date_end.split('T', 1)\n",
    "                else:\n",
    "                    end_date = end_time = ''\n",
    "                # Log successful extraction\n",
    "                print(f\"[SUCESS] {target} | RA={ra} | Dec={dec} | Date Start={obs_date} | Time Start={obs_time} | Date End={end_date} | Time End={end_time}\")\n",
    "            else:\n",
    "                print(f\"[ERROR] No file found for {target}\")\n",
    "                ra = dec = obs_date = obs_time = end_date = end_time = ''\n",
    "\n",
    "            # Append new values to the row\n",
    "            row['RA'] = ra\n",
    "            row['Dec'] = dec\n",
    "            row['Date Start'] = obs_date\n",
    "            row['Date End'] = end_date\n",
    "            row['Time Start'] = obs_time\n",
    "            row['Time End'] = end_time\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"[COMPLETE] Updated {csv_file}\")\n",
    "\n",
    "\n",
    "def main_update():\n",
    "    # Extract header info\n",
    "    update_catalog()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516bb791-c364-4ab2-b98c-40654629d1db",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 4: Calculate Median Time (for plot display)\n",
    "Takes the median time from Time Start and Time End to get the median\n",
    "observation time for single display on charts\n",
    "\"\"\"\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "def compute_med_time(csv_file=output_csv):\n",
    "    \"\"\"\n",
    "    Compute median of Time Start and Time End and append with a Median\n",
    "    Time column\n",
    "    \"\"\"\n",
    "    # Read existing CSV into memory\n",
    "    with open(csv_file, newline='') as f_in:\n",
    "        reader = csv.DictReader(f_in)\n",
    "        rows = list(reader)\n",
    "        # Determine if column exists\n",
    "        has_median = 'Median Time' in reader.fieldnames\n",
    "        fieldnames = reader.fieldnames + (['Median Time'] if not has_median else [])\n",
    "\n",
    "    # Parse time string (with or without microseconds)\n",
    "    def parse_time(ts):\n",
    "        for fmt in (\"%H:%M:%S.%f\", \"%H:%M:%S\"):\n",
    "            try:\n",
    "                return datetime.datetime.strptime(ts, fmt)\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return None\n",
    "\n",
    "    # Compute median time for each row\n",
    "    for row in rows:\n",
    "        target = row.get('Target', '')\n",
    "        existing = row.get('Median Time', '')\n",
    "        # Skip if already has data\n",
    "        if existing:\n",
    "            print(f\"[SKIP] Median Time already computed for {target}\")\n",
    "            continue\n",
    "        ts = row.get('Time Start', '')\n",
    "        te = row.get('Time End', '')\n",
    "        # Error if missing time data\n",
    "        if not ts or not te:\n",
    "            print(f\"[WARN] No time data for {target}\")\n",
    "            row['Median Time'] = ''\n",
    "            continue\n",
    "        # Parse start and end times\n",
    "        dt_s = parse_time(ts)\n",
    "        dt_e = parse_time(te)\n",
    "        if dt_s is None or dt_e is None:\n",
    "            print(f\"[WARN] Unrecognized time format: {ts} / {te}\")\n",
    "            row['Median Time'] = ''\n",
    "            continue\n",
    "        # Compute seconds since midnight\n",
    "        sec_s = dt_s.hour * 3600 + dt_s.minute * 60 + dt_s.second + dt_s.microsecond/1e6\n",
    "        sec_e = dt_e.hour * 3600 + dt_e.minute * 60 + dt_e.second + dt_e.microsecond/1e6\n",
    "        avg_sec = (sec_s + sec_e) / 2\n",
    "        # Convert back to HH:MM:SS\n",
    "        h = int(avg_sec // 3600)\n",
    "        m = int((avg_sec % 3600) // 60)\n",
    "        s = int(avg_sec % 60)\n",
    "        median_str = f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "        row['Median Time'] = median_str\n",
    "        print(f\"[SUCCESS] {row.get('Target')} | Median Time={median_str}\")\n",
    "\n",
    "    # Append catalog with updated data\n",
    "    with open(csv_file, 'w', newline='') as f_out:\n",
    "        writer = csv.DictWriter(f_out, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in rows:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"[COMPLETE] Updated {csv_file}\")\n",
    "\n",
    "\n",
    "def main_median():\n",
    "    # Compute median times\n",
    "    compute_med_time()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main_median()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
